{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "* Features engineering\n",
    "  * Lag (DONE)\n",
    "  * Lead (DONE)\n",
    "  * Overall aggregation\n",
    "  * Rolling aggregation\n",
    "* Modeling\n",
    "  * Linear Regression\n",
    "  * LightGBM -> optimize MAE, MAPE\n",
    "* Missing imputation\n",
    "  * Keep\n",
    "  * Fill with previous + add new column factor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Main package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Utility\n",
    "from itertools import combinations, product\n",
    "import random\n",
    "import calendar\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 200)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a utility functions first"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Default parameters\n",
    "TARGET = ['case', 'unit_cost']\n",
    "SEED = 2021\n",
    "categorical_features = [\n",
    "    'kddati2',\n",
    "    'tkp',\n",
    "    'id'\n",
    "]\n",
    "categorical_features_2 = [\n",
    "    'a', 'b', 'c', 'cb', 'd', 'ds', 'gd', 'hd', \n",
    "    'i1', 'i2', 'i3', 'i4', \n",
    "    'kb', 'kc', 'kg', 'ki', 'kj', 'kk', 'kl', 'km', 'ko', 'kp', 'kt', 'ku', \n",
    "    's', 'sa', 'sb', 'sc', 'sd'\n",
    "]\n",
    "remove_features = ['tglpelayanan','row_id','cat']\n",
    "numerical_features = ['peserta'] + TARGET + ['case2', 'unit_cost2']\n",
    "\n",
    "# Utility\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read data\n",
    "train = pd.read_csv('data/case_cost_prediction_train.csv')\n",
    "test = pd.read_csv('data/case_cost_prediction_val.csv')\n",
    "df = pd.concat([train.assign(cat = 'Train'), test.assign(cat = 'Test')])\n",
    "df = df.sort_values(['kddati2','tkp','tglpelayanan'])\n",
    "\n",
    "# Simple features engineering\n",
    "df['tglpelayanan'] = pd.to_datetime(df['tglpelayanan'])\n",
    "df['month'] = df['tglpelayanan'].dt.month\n",
    "df['year'] = df['tglpelayanan'].dt.year\n",
    "\n",
    "# Add combined ID\n",
    "df['id'] = df['kddati2'].astype(str) + '-' + df['tkp'].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Join with df_temp with different tkp\n",
    "df_temp = df[['tglpelayanan','kddati2','tkp','case','unit_cost']].copy()\n",
    "df_temp['tkp'] = np.where(df_temp['tkp'] == 30, 40, 30)\n",
    "df_temp = df_temp.rename(columns={'case':'case2', 'unit_cost':'unit_cost2'})\n",
    "df = df.merge(df_temp,how='left')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate lag/lead features\n",
    "df = df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in [1,-1]\n",
    "        for col in numerical_features + ['tglpelayanan']\n",
    "    })\n",
    "lag_features = [col for col in df.columns if 'lag' in col]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nlag_features = [col for col in lag_features if 'lag_1' in col and 'tglpelayanan' not in col and 'peserta' not in col]\n",
    "nlead_features = [col for col in lag_features if 'lag_-1' in col and 'tglpelayanan' not in col and 'peserta' not in col]\n",
    "print(nlag_features)\n",
    "print(nlead_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i, col in enumerate(['case2']):\n",
    "    df['ds_' + col] = np.where(df[col].isnull(), None, df['tglpelayanan'].astype(str))\n",
    "    df['ds_' + col] = df.groupby(['id'])['ds_' + col].ffill()\n",
    "    df['ds_' + col] = np.round(((df['tglpelayanan'] - pd.to_datetime(df['ds_' + col]))/np.timedelta64(1, 'M')))\n",
    "df[['case2','unit_cost2']] = df.groupby(['id'])['case2','unit_cost2'].ffill()\n",
    "\n",
    "for i, col in enumerate(['case_lag_1','case2_lag_1']):\n",
    "    df['ds_' + col] = np.where(df[col].isnull(), None, df['tglpelayanan_lag_1'].astype(str))\n",
    "    df['ds_' + col] = df.groupby(['id'])['ds_' + col].ffill()\n",
    "    df['ds_' + col] = np.round(((df['tglpelayanan'] - pd.to_datetime(df['ds_' + col]))/np.timedelta64(1, 'M')))\n",
    "df[nlag_features] = df.groupby(['id'])[nlag_features].ffill()\n",
    "\n",
    "for i, col in enumerate(['case_lag_-1','case2_lag_-1']):\n",
    "    df['ds_' + col] = np.where(df[col].isnull(), None, df['tglpelayanan_lag_-1'].astype(str))\n",
    "    df['ds_' + col] = df.groupby(['id'])['ds_' + col].bfill()\n",
    "    df['ds_' + col] = np.round(((df['tglpelayanan'] - pd.to_datetime(df['ds_' + col]))/np.timedelta64(1, 'M')))\n",
    "df[nlead_features] = df.groupby(['id'])[nlead_features].bfill()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.drop(columns=['tglpelayanan_lag_1','tglpelayanan_lag_-1'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def process_train(\n",
    "    df,\n",
    "    target=TARGET[0],\n",
    "    cv_split=5,\n",
    "    seed=SEED,\n",
    "    verbose=500\n",
    "):\n",
    "    df = df.copy()\n",
    "    df = df[df['cat'] == 'Train']\n",
    "    local_params = lgb_params.copy()\n",
    "\n",
    "    # Categorical feature\n",
    "    for col in categorical_features:\n",
    "        try:\n",
    "            df[col] = df[col].astype('category')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    all_features = [col for col in list(df) if col not in (remove_features + TARGET)]\n",
    "    folds = StratifiedKFold(n_splits=cv_split, random_state=SEED, shuffle=True)\n",
    "\n",
    "    print(all_features)\n",
    "    \n",
    "    pred_overall = pd.DataFrame()\n",
    "    for i, (train_index, val_index) in enumerate(folds.split(df, df['id'])):\n",
    "        train_data = lgb.Dataset(df.iloc[train_index][all_features], label=df.iloc[train_index][target])\n",
    "        val_data = lgb.Dataset(df.iloc[val_index][all_features], label=df.iloc[val_index][target])\n",
    "        temp_df = df.iloc[val_index]\n",
    "\n",
    "        print('\\nCV-{}'.format(i+1))\n",
    "        seed_everything()\n",
    "        estimator = lgb.train(local_params,\n",
    "                              train_data,\n",
    "                              valid_sets = [train_data, val_data],\n",
    "                              verbose_eval = verbose)\n",
    "\n",
    "        temp_df['pred'] = estimator.predict(temp_df[all_features])\n",
    "        temp_df = temp_df[['row_id','tglpelayanan','kddati2','tkp',target,'pred']]\n",
    "\n",
    "        print('MAPE CV-{} is {:.2f}%'.format(i+1, mape(temp_df[target], temp_df['pred'])))\n",
    "        print('MAE CV-{} is {:.2f}'.format(i+1, mae(temp_df[target], temp_df['pred'])))\n",
    "        \n",
    "        pred_overall = pred_overall.append(temp_df)\n",
    "        pred_overall['cv'] = i+1\n",
    "\n",
    "    print('\\nMAPE CV Overall is {:.2f}%'.format(mape(pred_overall[target], pred_overall['pred'])))\n",
    "    print('MAE CV Overall is {:.2f}'.format(mae(pred_overall[target], pred_overall['pred'])))\n",
    "\n",
    "    return estimator, pred_overall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgb_params = {'boosting_type': 'gbdt', \n",
    "              'objective': 'mape',\n",
    "              # 'objective': 'mape',\n",
    "              'metric': ['mae'], \n",
    "              'learning_rate': 0.1,      \n",
    "              'subsample': 0.9,       \n",
    "              'subsample_freq': 1,     \n",
    "              'num_leaves': 255,            \n",
    "              'min_data_in_leaf': 255, \n",
    "              'feature_fraction': 0.5,\n",
    "              'n_estimators': 500,   \n",
    "              'seed': SEED,\n",
    "              'verbose': -1}\n",
    "mdl, val_df = process_train(df, cv_split=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "source": [
    "lgb_params = {'boosting_type': 'gbdt', \n",
    "              'objective': 'mae',\n",
    "              # 'objective': 'mape',\n",
    "              'metric': ['mae'], \n",
    "              'learning_rate': 0.1,      \n",
    "              'subsample': 0.9,       \n",
    "              'subsample_freq': 1,     \n",
    "              'num_leaves': 255,            \n",
    "              'min_data_in_leaf': 255, \n",
    "              'feature_fraction': 0.25,\n",
    "              'n_estimators': 5000,   \n",
    "              'seed': SEED,\n",
    "              'verbose': -1}\n",
    "mdl, val_df = process_train(df.drop(columns=categorical_features_2), cv_split=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['kddati2', 'tkp', 'peserta', 'month', 'year', 'id', 'case2', 'unit_cost2', 'peserta_lag_1', 'case_lag_1', 'unit_cost_lag_1', 'case2_lag_1', 'unit_cost2_lag_1', 'peserta_lag_-1', 'case_lag_-1', 'unit_cost_lag_-1', 'case2_lag_-1', 'unit_cost2_lag_-1', 'ds_case2', 'ds_case_lag_1', 'ds_case2_lag_1', 'ds_case_lag_-1', 'ds_case2_lag_-1']\n",
      "\n",
      "CV-1\n",
      "[500]\ttraining's l1: 537.968\tvalid_1's l1: 662.312\n",
      "[1000]\ttraining's l1: 486.268\tvalid_1's l1: 633.133\n",
      "[1500]\ttraining's l1: 460.92\tvalid_1's l1: 618.311\n",
      "[2000]\ttraining's l1: 443.93\tvalid_1's l1: 608.224\n",
      "[2500]\ttraining's l1: 430.667\tvalid_1's l1: 599.91\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-9ce93ca73d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0;34m'seed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               'verbose': -1}\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_features_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-300-0d669463fb20>\u001b[0m in \u001b[0;36mprocess_train\u001b[0;34m(df, target, cv_split, seed, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCV-{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         estimator = lgb.train(local_params,\n\u001b[0m\u001b[1;32m     33\u001b[0m                               \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                               \u001b[0mvalid_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   1975\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgb.plot_importance(mdl)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_df.sort_values(['kddati2','tkp','tglpelayanan'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mape(val_df['case'], val_df['pred'] * 0.975)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgb_params = {'boosting_type': 'gbdt', \n",
    "              'objective': 'regression',\n",
    "              # 'objective': 'mape',\n",
    "              'metric': ['mae'], \n",
    "              'learning_rate': 0.05,      \n",
    "              'subsample': 0.9,       \n",
    "              'subsample_freq': 1,     \n",
    "              'num_leaves': 255,            \n",
    "              'min_data_in_leaf': 255, \n",
    "              'feature_fraction': 0.9,\n",
    "              'n_estimators': 500,   \n",
    "              'seed': SEED,\n",
    "              'verbose': -1}\n",
    "mdl, val_df = process_train(df.drop(columns=categorical_features_2), target=TARGET[1], cv_split=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgb.plot_importance(mdl)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgb_params = {'boosting_type': 'gbdt', \n",
    "              'objective': 'regression',\n",
    "              # 'objective': 'mape',\n",
    "              'metric': ['mae'], \n",
    "              'learning_rate': 0.025,      \n",
    "              'subsample': 0.9,       \n",
    "              'subsample_freq': 1,     \n",
    "              'num_leaves': 255,            \n",
    "              'min_data_in_leaf': 500, \n",
    "              'feature_fraction': 0.95,\n",
    "              'n_estimators': 1000,   \n",
    "              'seed': SEED,\n",
    "              'verbose': -1}\n",
    "mdl, val_df = process_train(df[['kddati2','tkp','unit_cost_lag_1','unit_cost_lag_-1','cat','id','unit_cost','row_id','tglpelayanan']], target=TARGET[1], cv_split=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgb_params = {'boosting_type': 'gbdt', \n",
    "              'objective': 'mae',\n",
    "              # 'objective': 'mape',\n",
    "              'metric': ['mae'], \n",
    "              'learning_rate': 0.025,      \n",
    "              'subsample': 0.9,       \n",
    "              'subsample_freq': 1,     \n",
    "              'num_leaves': 255,            \n",
    "              'min_data_in_leaf': 500, \n",
    "              'feature_fraction': 0.95,\n",
    "              'n_estimators': 1000,   \n",
    "              'seed': SEED,\n",
    "              'verbose': -1}\n",
    "mdl, val_df = process_train(df[['kddati2','tkp','unit_cost_lag_1','unit_cost_lag_-1','cat','id','unit_cost','row_id','tglpelayanan']], target=TARGET[1], cv_split=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9f92744e12c76bcce0c7412a206e46c8bf1a54cc67d5f25e706318e33507cbb7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}